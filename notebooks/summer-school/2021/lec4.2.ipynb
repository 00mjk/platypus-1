{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Classical Machine Learning:\n",
    "\n",
    "Classical Machine Learning Models - Part 2\n",
    "\n",
    "<div class=\"youtube-wrapper\">\n",
    "    <iframe src=\"https://www.youtube.com/embed/lpPij21jnZ4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>\n",
    "\n",
    "\n",
    "This is the second session of the classical machine learning (ML) introduction presented by Amira.  A brief of history of ML was presented followed up by the introduction of the simplest ML methods, i.e., linear regression based models. Neural networks were introduced started with simple perceptron and then illustrating Feed Forward Neural Networks (FFNNs). In the final section, for classification type tasks Supper Vector Machines (SVM) concepts were introduced. Also, a very high level overview of Quantum Machine Learning was introduced with the quadrants showing both classical and quantum machine learning in terms of Data Processing device as one axis and the Data generating system as the remaining axis.\n",
    "\n",
    "​\n",
    "\n",
    "### Suggested links\n",
    "\n",
    " - Download the lecturer's notes [here](Download Lecture Notes: Advanced Classical Machine Learning\n",
    "\n",
    "https://github.com/justjosie/event-info-and-assets/raw/master/2021/QGSS2021/Global/LectureNotes/QGSS2021_Lecture4.2_LectureNotes.pdf)\n",
    "\n",
    "### Other resources\n",
    "\n",
    "### Official Suggested Reading\n",
    "Deep Learning Book\n",
    "\n",
    "by Ian Goodfello et al.\n",
    "\n",
    "Read Here -->\n",
    "\n",
    "Available for Purchase Here -->\n",
    "\n",
    "​\n",
    "\n",
    "Neural Network Youtube Series\n",
    "\n",
    "by 3blue1brown\n",
    "\n",
    "Watch Here -->\n",
    "\n",
    "### (1) Lecture Q&A: The following questions and answers (if provided) were captured from the Ask a Question window of the Lecture.\n",
    "(Note: questions without answers imply no answers were posted during the lecture in the Ask a Queston window)\n",
    "\n",
    "### Q: How easy is it to invert a linear model, i.e. to invert the role of the input and the output? Or for multiple features, invert the role of the input and one of the outputs? (received 4 votes for question order ranking)\n",
    "<<<No Answer provided during the session>>>\n",
    "\n",
    "​\n",
    "\n",
    "### Q: Why using linear model not non-linear model? Can we decide the boundary like y=theta*X^2? (received 1 vote)\n",
    "<<<No Answer provided during the session>>>\n",
    "\n",
    "​\n",
    "\n",
    "### Q: How are linear models related to the standard statistical methods of least squares for regression analysis? (received 1 vote)\n",
    "A: They are the same\n",
    "\n",
    "### Q: Are the activation functions in each layer the same or different? (received 1 vote)\n",
    "A:  Several answers provided\n",
    "\n",
    "<div class=\"\" data-section-style=\"5\" style=\"\"><ul id=\"COMACAA9gmM\"><li class=\"\" id=\"COMACAQTBS1\" value=\"1\"><span id=\"COMACAQTBS1\"><a href=\"https://www.crowdcast.io/bonifaceyogendran\">Boniface Yogendran<b> </b></a><span style=\"color:#777777\" textcolor=\"#777777\">It can be differnt</span></span>\n",
    "<br/></li><li class=\"\" id=\"COMACACkh0e\"><span id=\"COMACACkh0e\"><a href=\"https://www.crowdcast.io/userd7899615\">Edwin<b> </b></a><span style=\"color:#777777\" textcolor=\"#777777\">Usually last activation function is different rest all are kept same.</span></span>\n",
    "<br/></li></ul></div>\n",
    "​\n",
    "\n",
    "### Q: Just confirming...The more data there is the easier the higher accuracy the model has? How to combat lack of available data especialy when create a CNN network?\n",
    "A: One way to combat is replicating present data and augment that data. Maybe vertical flip, horizontal flip, crop, blur, etc.\n",
    "\n",
    "​\n",
    "\n",
    "### Q: Do linear models in general get more/less accurate when taking more features into account?\n",
    "A: If you'll take more more features, it'll result in overfitting. If less, then you'll have biased output.\n",
    "\n",
    "​\n",
    "\n",
    "### Q: On the regression model, does theta include the magnitude of the intercept?\n",
    "A:  Yes, its \\theta_0\n",
    "\n",
    "​\n",
    "\n",
    "### Q: During the lecture it was stated that one can easily add or out a bias, how does this work? Could you please explain?\n",
    "A1: For example, think about on a linear model, you can change the value of the intercept to move the boundary within the plane.\n",
    "\n",
    "A2: The main function of a bias is to provide every node with a trainable constant value (in addition to the normal inputs that the node receives). You can achieve that with a single bias node with connections to N nodes, or with N bias nodes each with a single connection; the result should be the same\n",
    "\n",
    "### Q: Is it important that the nonlinearities are multiplied after each matrix multiplication or could they be pulled out via commutativity and multiplied after all the matrix mults (depending on the number of layers) are done?\n",
    "<<<No Answer provided during the session>>>\n",
    "\n",
    "​\n",
    "\n",
    "### Q: Is it correct to think of neural networks (NN) as models that can be used for non-linear fitting, in general? Would it an overkill to use NNs for datasets with linear dependencies?\n",
    "A:  NN requires lots of data and computationally expensive. If its a linear model, we can get away with less computationally expensive ML methods.\n",
    "\n",
    "​\n",
    "\n",
    "### Q: Is there a rule of thumb in the selection of the weights and thus the number of neurons?\n",
    "A:  Actually in practice, number of neurons in each layer is 2^x. Number of layers is your call. Weights are automatically selected for you during backpropagation (optimization). You just initialize weight vector with random numbers. Thats it.\n",
    "\n",
    "### Q: Does a FFNN always use a liner model with respect to the weights?\n",
    "<<<No Answer provided during the session>>>\n",
    "\n",
    "​\n",
    "\n",
    "### Q: Does noise affect all these models?\n",
    "A:  Yes it does. It won't if your model is generalized aptly (which hasn't been achieved yet as still in one way or the other, models are affected by some kind of a noise)\n",
    "\n",
    "​\n",
    "\n",
    "### Q: Can we use a circular or elliptical function instead of a feature map for data that is not separable linearly?\n",
    "A:  Yes. feature mapping is eassier.\n",
    "\n",
    "​\n",
    "\n",
    "### (2) Followup live Q&A Session (starting from questions with highest number of votes)\n",
    "Q: What are the different ways to map the data in higher dimensions?\n",
    "\n",
    "A: Answer was provided at timestamp 2m 29s in the Lecture 4.2 Live Q&A session\n",
    "\n",
    "​\n",
    "\n",
    "Q: Could you please explain again what a kernel is?\n",
    "\n",
    "A: Answer was provided at timestamp 4m 48s in the Lecture 4.2 Live Q&A session\n",
    "\n",
    "​\n",
    "\n",
    "Q: How do we decide the Bias ? and does it remain same throughout training?\n",
    "\n",
    "A: Answer was provided at timestamp 9m 30s in the Lecture 4.2 Live Q&A session\n",
    "\n",
    "​\n",
    "\n",
    "Q: Someone in the previous video wanted a bit more clarification on why the dual formulation is useful\n",
    "\n",
    "A: Answer was provided at timestamp 10m 40s in the Lecture 4.2 Live Q&A session\n",
    "\n",
    "​\n",
    "\n",
    "Q: What if, after applying the feature map, data is still not linearly separable?\n",
    "\n",
    "A: Answer was provided at timestamp 13m 4s in the Lecture 4.2 Live Q&A session\n",
    "\n",
    "​\n",
    "\n",
    "Q: Why do we not have a different activation function for each parameter set ?\n",
    "\n",
    "A: Answer was provided at timestamp 14m 46s in the Lecture 4.2 Live Q&A session\n",
    "\n",
    "​\n",
    "\n",
    "Q: How can we choose a good activation function for a FFNN ?\n",
    "\n",
    "A: Answer was provided at timestamp 17m 21s in the Lecture 4.2 Live Q&A session\n",
    "\n",
    "​\n",
    "\n",
    "Q: How do we incorporate the optimization of the distance between the linear model and nearest points in each class into the SVM ?\n",
    "\n",
    "A: Answer was provided at timestamp 20m 5s in the Lecture 4.2 Live Q&A session\n",
    "\n",
    "​\n",
    "\n",
    "View Answer\n",
    "\n",
    "Suggested Resources: Websites/Videos\n",
    "\n",
    "Artificial Intelligence: A General Survey (Lighthill Report)\n",
    "\n",
    "James Lighthill (1973)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Lighthill_report\n",
    "\n",
    "​\n",
    "\n",
    "Algoithm Kumulatiivinen Pyöristysvirhe\n",
    "\n",
    "Seppo Linnainmaa Thesis (1970)\n",
    "\n",
    "https://people.idsia.ch//~juergen/linnainmaa1970thesis.pdf\n",
    "\n",
    "​\n",
    "\n",
    "Learning: Support Vector Machines\n",
    "\n",
    "MIT Opencourseware\n",
    "\n",
    "https://www.youtube.com/watch?v=_PwhiWxHK8o&t=1123s\n",
    "\n",
    "​\n",
    "\n",
    "Glossary for terminology used in Machine Learning versus Statistics\n",
    "\n",
    "Stanford University Statistics 315a Modern Applied Statistics course material\n",
    "\n",
    "http://statweb.stanford.edu/~tibs/stat315a/glossary.pdf\n",
    "\n",
    "​\n",
    "\n",
    "Support Vector Machine: Complete Theory\n",
    "\n",
    "Saptashwa Bhattacharyya\n",
    "\n",
    "https://towardsdatascience.com/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e\n",
    "\n",
    "​\n",
    "\n",
    "Support Vector Machine: Kernel Trick; Mercer’s Theorem\n",
    "\n",
    "Saptashwa Bhattacharyya\n",
    "\n",
    "https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d\n",
    "\n",
    "Note: above link was posted as useful link for visual representation\n",
    "\n",
    "​\n",
    "\n",
    "Support Vector Machines (SVM) Tutorial\n",
    "\n",
    "Zoya Gavrilov at MIT\n",
    "\n",
    "https://web.mit.edu/zoya/www/SVM.pdf\n",
    "\n",
    "​\n",
    "\n",
    "Improving and automating quantum computers with machine learning\n",
    "\n",
    "Michael J. Biercuk\n",
    "\n",
    "https://youtu.be/G_UMvI2bASg\n",
    "\n",
    "​\n",
    "\n",
    "Feature Maps\n",
    "\n",
    "Qiskit\n",
    "\n",
    "https://qiskit.org/documentation/apidoc/qiskit.aqua.components.feature_maps.html\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "### Suggested Resources on Introductory Machine Learning\n",
    "Machine Learning Crash Course with TensorFlow APIs\n",
    "\n",
    "Google\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course\n",
    "\n",
    "​\n",
    "\n",
    "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\n",
    "\n",
    "https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\n",
    "\n",
    "​\n",
    "\n",
    "Deep Learning Specialization\n",
    "\n",
    "https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome\n",
    "\n",
    "​\n",
    "\n",
    "Google ML Crash Course\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/ml-intro\n",
    "\n",
    "​\n",
    "\n",
    "MIT Introduction to Deep Learning\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi\n",
    "\n",
    "​\n",
    "\n",
    "TensorFlow Tutorials\n",
    "\n",
    "https://www.tensorflow.org/tutorials\n",
    "\n",
    "​\n",
    "\n",
    "Stanford ML Course\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning\n",
    "\n",
    "​\n",
    "\n",
    "3Blue1Brown series on Neural Nets (Gradient descent, how neural networks learn | Chapter 2, Deep learning)\n",
    "\n",
    "https://www.youtube.com/watch?v=IHZwWFHWa-w\n",
    "\n",
    "​\n",
    "\n",
    "Tinker With a Neural Network Right Here in Your Browser.\n",
    "\n",
    "https://playground.tensorflow.org\n",
    "\n",
    "​\n",
    "\n",
    "Comparison of activation functions\n",
    "\n",
    "https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions\n",
    "\n",
    "​\n",
    "\n",
    "Detailed responses in Stackoverflow on “What is the role of the bias in neural networks?“\n",
    "\n",
    "https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks\n",
    "\n",
    "​\n",
    "\n",
    "Pattern Recognition and Machine Learning (by Christopher Bishop)\n",
    "\n",
    "https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/\n",
    "\n",
    "​\n",
    "\n",
    "StatQuest with Josh Starmer\n",
    "\n",
    "https://www.youtube.com/c/joshstarmer\n",
    "\n",
    "​\n",
    "\n",
    "Deep Learning related introductory content by Jon Krohn\n",
    "\n",
    "https://www.jonkrohn.com/\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
